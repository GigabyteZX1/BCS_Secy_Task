{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da23be91-4e88-4c0d-b062-a2b5bacb1692",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f31dd2-a609-4ce8-9c03-37318b7aaaac",
   "metadata": {},
   "source": [
    "In Layman's terms Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI, machine learning, and deep learning. They are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "Technically speaking Artificial neural networks (ANNs) (another name for Neural Networks) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.Neural networks rely on training data to learn and improve their accuracy over time.\n",
    "After a neural network is fine tuned and successfully trained, it can then be used to solve real world problems like classification of images, prediction of different parameters be it weather forecast, market etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c348f74-cd9a-469e-9af9-ad69eb088c7c",
   "metadata": {},
   "source": [
    "# How do Neural Networks work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f2494-bb48-4fc3-839c-d0fb0a3340de",
   "metadata": {},
   "source": [
    "To understand how a neural network work we can think of each individual node (smallest unit of a neural network) as a complex model consisting of input data, weights, a bias (or threshold), and an output. The formula would look something like this:\n",
    "              <blockquote>∑wixi + bias = w1x1 + w2x2 + w3x3 + bias </blockquote>\n",
    "             <blockquote> output = f(x) = 1 if ∑w1x1 + b >= 0;\n",
    "                                 0 if ∑w1x1 + b < 0 </blockquote>\n",
    "Once an input layer is determined, weights are assigned at random at first, then as the model is trained these weights are also fine tuned to given the most preferable outcome. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88a19f-0a47-4fc1-b5e8-2bb7095061ff",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6f6ca-a66f-42bf-8637-0b5c89465791",
   "metadata": {},
   "source": [
    "Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21def71f-7f67-405f-a53a-8405e4a01980",
   "metadata": {},
   "source": [
    "### Some Activation Functions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5aa6c-4e84-4de2-8f62-f6b03a31d29d",
   "metadata": {},
   "source": [
    "#### Sigmoid Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26adb625-18f8-4a9f-856f-cc9b389f06e6",
   "metadata": {},
   "source": [
    "It is a function which is plotted as ‘S’ shaped graph.\n",
    "Equation :\n",
    "A = 1/(1 + e-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a461d-ca4a-4d37-9edd-f7297514e8c0",
   "metadata": {},
   "source": [
    "#### Tanh Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a337fc6-bc23-4df3-840a-d025f750fefb",
   "metadata": {},
   "source": [
    "The activation that works almost always better than sigmoid function is Tanh function also knows as Tangent Hyperbolic function. It’s actually mathematically shifted version of the sigmoid function.\n",
    ">f(x) = tanh(x) = 2/(1 + e^-2x) - 1 <br>\n",
    "OR <br>\n",
    "tanh(x) = 2 * sigmoid(2x) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4660d5-dba4-43c9-b8fd-dc2481107cc6",
   "metadata": {},
   "source": [
    "#### RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e057b-1118-4c72-b177-b27d36a251e8",
   "metadata": {},
   "source": [
    "Stands for Rectified linear unit. It is the most widely used activation function. Chiefly implemented in hidden layers of Neural network. <br>\n",
    "A(x) = max(0,x). It gives an output x if x is positive and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d86f8d-be26-4ade-8a03-fe0573b13dad",
   "metadata": {},
   "source": [
    "The basic rule of thumb is if you really don’t know what activation function to use, then simply use RELU as it is a general activation function and is used in most cases these days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72d5c5-6020-4b6a-8dba-aef4de02cd6e",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d2bab-d53a-402c-8d76-66cefbffbf7f",
   "metadata": {},
   "source": [
    "A Cost Function is used to measure just how wrong the model is in finding a relation between the input and output. It tells you how badly your model is behaving/predicting.Or how far is your predicted output is from ground truth. <br>\n",
    "There are many different types of Cost function available like CrossEntropyLoss, NegativeLogLikelihood loss etc and choosing the correct function according to the problem requirement is also very important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473cb40-0b77-4579-ba8d-1d07837c0d25",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0c06c-29bb-4c47-a43b-c49f35fde7f6",
   "metadata": {},
   "source": [
    "Gradient Descent is an algorithm that is used to optimize the cost function or the error of the model. It is used to find the minimum value of error possible in your model.Gradient Descent can be thought of as the direction you have to take to reach the least possible error. The error in your model can be different at different points, and you have to find the quickest way to minimize it, to prevent resource wastage.<br>\n",
    "In gradient descent, you find the error in your model for different values of input variables. This is repeated, and soon you see that the error values keep getting smaller and smaller. Soon you’ll arrive at the values for variables when the error is the least, and the cost function is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42ebd6-2892-4f57-a8a0-961521d9387d",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8acc8-c18d-4b01-aa12-80274eee9705",
   "metadata": {},
   "source": [
    "Backpropagation is the essence of neural network training. It is the method of fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration. Proper tuning of the weights allows you to reduce error rates and make the model reliable by increasing its generalization. <br>\n",
    "Backpropagation in neural network is a short form for “backward propagation of errors.” It is a standard method of training artificial neural networks. This method helps calculate the gradient of a loss function with respect to all the weights in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25e44d-0e57-40b2-96e9-cae4c227714f",
   "metadata": {},
   "source": [
    "#### How Backpropagation works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f7ce9-dde9-402b-b4cb-8d5be0979574",
   "metadata": {},
   "source": [
    "At the heart of backpropagation is an expression for the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. <br>\n",
    "So as to not make this very overwhelming to understand in simple terms we can understang the working of Backpropagation as follows: <br>\n",
    "- We run the model with randomised weights that means data is passed through all the layers and an error is calculated using cost function. <br>\n",
    "- We then take partial derivative of cost function wrt different weights, this quantity is used to change weights values <br>\n",
    "- The model is again run with these modified weights and data is passed through all the layers and a cost is again calculated<br>\n",
    "- This process is repeated until we get minimum loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935160b1-0289-4c59-a5e6-c06284f0197d",
   "metadata": {},
   "source": [
    "# Below I have tried to build a CNN Model to classify MNIST Dataset using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e80e9-a1f8-4973-b6c7-1fb54308336f",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "410c2a0c-e16f-443f-ba0c-df2e603dbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b101c0c-c44a-4b31-9650-4307f23b362f",
   "metadata": {},
   "source": [
    "Checking to see if the GPU is available or not (if available use that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b103fd5d-0d38-4cd0-ac4f-0eb659734004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186a9a5-a94c-4658-9de8-19ff14a42d23",
   "metadata": {},
   "source": [
    "Downloading and loading the MNIST dataset in train and test loader with appropriate transforms and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd94d7d0-1fe7-4666-8eb4-09edd5f3320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1,), (0.5,))\n",
    "                             ])),\n",
    "  batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b4880-2965-4cf0-a786-85e7c2ba7ef9",
   "metadata": {},
   "source": [
    "Retrieving some example images to check if the data is loaded correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "584e227e-d078-40cf-bca4-ff5f2b5a63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data, example_targets = example_data.to(device), example_targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53b13f1f-f7fe-434e-973d-53d0f1c33233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb980b9-b9ad-4cbb-9554-194592a360de",
   "metadata": {},
   "source": [
    "Printing some images from the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5fed02e-65ee-4dac-8496-aea3dea3b2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0].cpu(), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Figure: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878dcf0e-13d2-40cb-8ed7-4a3dd5229389",
   "metadata": {},
   "source": [
    "Writing the Network/model which we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b84f5a2-4077-43d0-a490-6ec8e3d98d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.convolutaional_neural_network_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=1, stride=1), # (N, 1, 28, 28) \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "                nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, padding=1, stride=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2) \n",
    "        )\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                nn.Linear(in_features=24*7*7, out_features=64),          \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Linear(in_features=64, out_features=10)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass \n",
    "    def forward(self, x):\n",
    "        x = self.convolutaional_neural_network_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return F.log_softmax(x,dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd2ca6-fd7a-49e5-93b2-5c472037b276",
   "metadata": {},
   "source": [
    "Below we can see the number and types of different layers in the Network as well as the activation function used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc16a0d4-ef88-4bdb-8c6e-356ef1832d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (convolutaional_neural_network_layers): Sequential(\n",
      "    (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=1176, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = NN()\n",
    "network.to(device)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe1ac2-af1f-49a5-b200-642af7782b8b",
   "metadata": {},
   "source": [
    "Defining learning rate, optimizer and cost function used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0e556869-1e47-470e-a662-6f1be12127d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5429a-af9f-4774-82e6-a5ca1875be33",
   "metadata": {},
   "source": [
    "Some other hyperparameters for keeping track of loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d093af4-a6f0-4e17-98b7-bb803a983151",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37744b4a-3468-4b7e-8133-5b1a77899067",
   "metadata": {},
   "source": [
    "Defining training function, the general flow goes as follows:<br>\n",
    "- We start the trainig mode\n",
    "- We load the images and labels from train_loader\n",
    "- We set gradient to zero (so as to prevent gradient overlapping)\n",
    "- We run the data through each layer and calculate loss using cost function\n",
    "- This loss is then backpropagated to modify weight\n",
    "- Then optimizer is used to optimise the weights and run the model again\n",
    "- In between training we print the status of model in the form of loss at the end of each iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "20261238-ba0a-423b-b085-d4810f326808",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data,target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c4592-688e-4ed6-941b-a6922e610129",
   "metadata": {},
   "source": [
    "After training our model for some time we test it using eval mode and simply running data through each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ada83909-c263-44df-a031-2084731b54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data,target = data.to(device), target.to(device)\n",
    "      output = network(data)\n",
    "      test_loss += criterion(output, target).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  #test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9773e-2767-4e6d-9b75-556ca0afc197",
   "metadata": {},
   "source": [
    "Actual calling of test and train function. We check how model behaves without any training then see the improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e322a0a2-bf68-4244-aee4-845fbb17d6b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 23.1755, Accuracy: 1273/10000 (13%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316149\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.918007\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.283488\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.759766\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.553724\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.593842\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.516215\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.554239\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.684718\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.374137\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.373627\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.400426\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.476154\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.414827\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.435834\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.360325\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.231278\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.145352\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.182283\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.236794\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.305602\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.278138\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.269581\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.173606\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.319443\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.234876\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.175269\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.157729\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.264975\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.108202\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.284306\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.253999\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.172071\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.135510\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.288315\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.134525\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.249854\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.302419\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.134257\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.222308\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.168179\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.111340\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.118088\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.192197\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.127311\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.108844\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.120583\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.129068\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.037032\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.138429\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.035297\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.122455\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.133637\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.146321\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.052288\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.122293\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.138116\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.332098\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.139762\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.126491\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.148505\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.075348\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.152191\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.071716\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.076024\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.017332\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.046336\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.167990\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.079124\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.036784\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.115325\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.084987\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.102307\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.051142\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.117492\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.083445\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.178455\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.052379\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.087059\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.075150\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.040440\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.107641\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.055646\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.027276\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.182519\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.042094\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.052308\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.035130\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.146976\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.091282\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.123716\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.038345\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.059403\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.089827\n",
      "\n",
      "Test set: Avg. loss: 0.8987, Accuracy: 9807/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.044304\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.028806\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.058680\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.095322\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.059979\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.122856\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.199970\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.097809\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.058827\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.088055\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.072511\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.046312\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.021760\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.192755\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.112910\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.028761\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.119198\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.125403\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.053755\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.018684\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.007137\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.117072\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.020507\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.113074\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.163790\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.068680\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.083609\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.011358\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.032346\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.016970\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.066621\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.118333\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.023622\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.138972\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.158263\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.097293\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.133067\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.024890\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.053001\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.022906\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.061950\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.059916\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.033166\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.267519\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.023287\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.022849\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.023508\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.086012\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.027065\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.018370\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.019931\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.100187\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.069889\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.136416\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.026184\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.051734\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.012871\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.033397\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.237868\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.212829\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.129744\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.180187\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.046071\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.075912\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.024149\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.033633\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.084896\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.060082\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.009889\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.031829\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.052277\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.116570\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.105466\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.021373\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.048450\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.033483\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.033246\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.135769\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.105940\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.074401\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.047900\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.191759\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.038221\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.046378\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.026312\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.109080\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.114133\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.170664\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.048198\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.041785\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.051326\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.045850\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.081537\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.107656\n",
      "\n",
      "Test set: Avg. loss: 0.6353, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.047884\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.027406\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.098409\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.045120\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.005042\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.032938\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.006505\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.040989\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.013716\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.028724\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.035139\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.012388\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.117472\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.044614\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.014098\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.027509\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.188320\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.054373\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.096151\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.032639\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.027728\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.035162\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.024909\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.007929\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.034556\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.105450\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.035744\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.029240\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.046050\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.035926\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.053868\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.041392\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.064362\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.086373\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.043751\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.114970\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.029212\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.021274\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.021992\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.052647\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.057251\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.005910\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.207025\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.029642\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.012523\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.045663\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.015043\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.119782\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.013066\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.066124\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.060688\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.040492\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.054950\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.032459\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.039553\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.071078\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.083065\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.021570\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.016884\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.014080\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.008204\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.015423\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.085696\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.034984\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.058971\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.034043\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.016852\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.046159\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.006520\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.124812\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.019556\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.134431\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.173880\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.058299\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.098931\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.053592\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.039207\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.075533\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.006490\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.039785\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.109616\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.014072\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.024007\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.095149\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.146411\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.008664\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.040554\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.015459\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.060864\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.082292\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.068446\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.065144\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.148496\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.071911\n",
      "\n",
      "Test set: Avg. loss: 0.5365, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.082656\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.052067\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.014969\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.041204\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.015149\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.009319\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.032961\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.076650\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.109151\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.006831\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.010317\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.093615\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001921\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.062020\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.034471\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.058800\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.024490\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.017022\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.024854\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.036944\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.010261\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.045233\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.009851\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.034100\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.040811\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.032961\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.026802\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.013626\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.146684\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.065559\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.003822\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.057334\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.042508\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.112426\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.097307\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.005273\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.059217\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.079524\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.123973\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.015909\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.095159\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.009874\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.087214\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.016302\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.120794\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.022105\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.023099\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.082161\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.031980\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.039265\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.040864\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.020939\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.042273\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.005690\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.020365\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.005827\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.034942\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.009725\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.012200\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.070967\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.028153\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.020470\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.022804\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.008270\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.003075\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.029172\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.072609\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.158052\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.016420\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.020903\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.004593\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.011278\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.055197\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.011113\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.044741\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.040632\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.071046\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.074244\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.013253\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.031639\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.043545\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.034148\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.062518\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.057947\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.009559\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.025806\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.046044\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.025640\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.027336\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.086920\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.052360\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.033186\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.014389\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.096798\n",
      "\n",
      "Test set: Avg. loss: 0.4207, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.091006\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.024200\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.032151\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.007906\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.027917\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.015415\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.018717\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.016275\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.015417\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.017331\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.039221\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.002581\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.016613\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.002528\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.014844\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.099601\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.040399\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.101132\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.004534\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.033434\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.066075\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.116866\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.003941\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.075588\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.015747\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.048241\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.078192\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.009899\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.060482\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.021573\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.018280\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.009354\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.147619\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.020154\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.003494\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.005167\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.018911\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.017663\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.008438\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.067955\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034120\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.097521\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.004292\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.025367\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.082679\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.011393\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.065758\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.023658\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.079507\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.014807\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.008173\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.008100\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.007496\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.029908\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.047045\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.015591\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.016464\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.009275\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.041053\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.008878\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.029704\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.014608\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.053517\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.023133\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.001871\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.003564\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.005833\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.018390\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.020657\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.009753\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.038977\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.028871\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.004953\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.013379\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.048906\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.027241\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.045072\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.006260\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.005337\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.002834\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.027105\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.001567\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.036827\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.125388\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.074499\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.011056\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.002313\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.098726\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.014817\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.022458\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.019979\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.017635\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.047396\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.020838\n",
      "\n",
      "Test set: Avg. loss: 0.4683, Accuracy: 9891/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b42e39-6056-4dc0-b762-4c38b180acc2",
   "metadata": {},
   "source": [
    "After the training is complete we check if the model is predicting correctly or not using visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2aefc410-ee66-4d98-a52c-34f1581014a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = network(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1341371a-72c2-4ce6-b710-4d4c996478ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(12):\n",
    "  plt.subplot(4,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0].cpu(), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
